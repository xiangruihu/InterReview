说话人 2 06:41
哎，好的好的，挺好的，就是我们简单地探讨一些问题。嗯，好的，简单探讨一下。嗯，就是你刚才提到强化学习，嗯，就是强化学习里面有一个很重要的概念叫做贝尔曼方程，就是不知道你平时，嗯，你是怎么理解贝尔曼方程的？

说话人 1 07:03
贝尔曼方程我理解它的话是基于我的当前的一些类似于它是一个概率的一个方程，然后它是基于我们当前的一些策略或者当前的一些环境，以及我们历史的一些信息，去做一个相应的一些概率的成绩的一些方法，去得到它的一些决策和我们的一些当前的一些概率的一些转移的一些状态的一些过程，这是我的一些判断方程。

说话人 2 07:33
那听懂你没？太听懂你想说什么？不好意思，就是你，你就能不能稍微简练的一点给我概括一下？就贝尔曼方程说了什么？

说话人 1 07:43
贝尔曼方程的话主要是做的概率的一些转移的一些相关的一些计算，对。

说话人 2 07:49
他说了描述了一件什么样的事情。

说话人 1 07:53
这个具体的概念我可能具体描述的好像是当前概率转移到下一个概率的一些概率，就是根据前面那些历史事件转移到下一个事件或者下一个状态的一些概率。

说话人 2 08:11
那下一个事件的概率，这不是条件概率吗？

说话人 1 08:15
这块的话可能没有就有点忘了，对。

说话人 2 08:19
好，然后那你觉得就是比如说就强化学习里面有一些，比如说一些动态规划的一些算法，就是策略迭代、价值迭代，然后也有蒙特卡洛方法嘛。嗯，对，然后你觉得就是，比如说就是我觉得就是我们已经有了这些方法了，来降低就是求解贝尔曼方程的一些手段，那我们为什么还要引入这种持续差分算法？这种来帮助我们处理一些问题？就我们为什么会需要持续差分？

说话人 1 08:58
嗯嗯嗯，好的。我理解说为什么我们需要去做一些持续差分的方法去，嗯，解决这个，嗯，贝尔曼的一些问题，我理解是主要如果我们用嗯贝叶斯方程去做，估计它存在一些方差和偏差的一些问题，然后比如说我们用当前步去估计未来很多步的话，它可能存在一个嗯偏差较大，但是方差较小。然后如果说我们做很多步的一些估计，我们当前步估计很多步的话，它可能存在方差较大，但是偏差较小的问题，那么这是它主要用一些差分的方法去解决方差和偏差的一些问题，包括我们现在比如说在 p PPO 里面的话用了一些广，用了一些优势计算的，以及它的一些多步持续多步的一个 rule 的一些计算方法，然后都是解决这个问题。

说话人 2 09:53
他们就是比如说这个持续差分这个问题的设定。你知道是什么吗？就是他，他跟这个，就是他跟，就是比如说加，嗯这个动态规划这个问题的设定，是吧？怎么不一样？

说话人 1 10:15
他的问题的设定是我理解他是根据当前的步去估计未来的一些步的，比如说单步或者多步的一些奖励，或者说一个整体的 reward 的一个估计。

说话人 2 10:36
不能用蒙特卡洛，蒙特卡洛计算还更简单。

说话人 1 10:43
蒙特卡洛的话可能存在较大的方差或者是偏差。

说话人 2 10:54
好，然后你觉得就是比如说时序差分里面 true learning 跟 SARSA 它有什么区别？

说话人 1 11:13
q learning 的话它主要是学习一个类似于一个表格的形式，然后我们的未来的每一步都是固定的，然后我们来估计我们未来，或者说我们在一个有限的状态下去选择我们在下一步要进行的一些决策或些现。同学。

说话人 2 11:35
哎，同学，我们不能使用 AI 来帮助我们回答，哎。

说话人 1 11:39
不是我没有使用AI，不是我没有使用AI。

说话人 2 11:46
哦，好好，我那我跟你说。

说话人 1 11:49
对啊，你可能我，我看旁边，是吧？我手机在这里，对，我想思考时候我可能，对就是好，我现在尽量去面向你。对，首先 q learning 的话大概就说我需要去学习一个表的方式去来，因为我们未来每一步都是能够去。嗯，有一些一个固定的步骤，然后我们去学习这个表，以及我们在当前步去决策未来哪一个某一个状态时候，它的一个表格的一些选择。然后关于您说的另外一个算法的话，我可能没有太多了解，所以说我不做详细地概述了，对。

说话人 2 12:32
我们就对，然后简单地探讨一下这些大模型里面的一些问题。

说话人 1 12:39
好的，好的。

说话人 2 12:40
对，对。然后比如说我们在调用大模型的时候，就是我们肯定会调整大模型的温度，嗯，对。然后你觉得温度它是怎么控制大模型的输出的？

说话人 1 12:56
温度话，我们主要是控制它的一个输出的一个概率的一些输出的一个概率的分布。然后比如说我们主要它最后有一个计算 logics 的一个输出，然后输出它整体的一个概率的分布。然后温度我们主要是设置，比如说我们使用一些不同温度，主要是把它的一些不同的概率的值做一些放缩，然后类似于，我理解温度话类似于一种规划，或者说类似于一种调整整体的输出的一些数据的分布的一些方法，然后这是我对温度的一个理解，然后它具体的话，计算的话类似于我之前看过他们计算，具体计算公式我忘了，就是说调整它原有的一个数据的输出分布，比如说把原有的输出的概率较小的。我可以调整得相对较大，让它有一个较大的一个采样分布空间啊。

说话人 2 13:45
对，然后它越大的话它的分布会越怎么样？

说话人 1 13:50
我们输出的话主要是如果我们的输出的概率温度越低的话，让它的输出的越稳定，然后如果说我们输出的越大的话，它整个整体输出的就是越发散。对。

说话人 2 14:05
哎，你有没有思考过这样一个问题？如果你把我们这个温度设成0，它会。

说话人 1 14:10
怎么样？温度设成 0 的话，我们按照理论上它是能够去每次输入都是一致的。

说话人 2 14:17
对，是理论上应该是一致的，但你如果实际上使用的话，你会发现它最后输出还是有点不一样。对对对，不论是千问还有 DP 的那些模型，它的温度设成 0 之后它还是有点不一样。

说话人 1 14:34
你觉得这是为什么？这个从底层上讲，之前有篇博客叫 thinking machine，我觉得你应该看过这篇，他们发的一篇博客，就说上面写的说为什么我们推理过程中，或者说在即使温度设置为0，然后我们采用最大，不采用采样的过程中采用最大概率的一个采样过程，它生成的结果也是零的一些情况。然后主要是我们在底层的话，我们在做。计算的时候可能存在，比如说我们计算过程中它的一些采用底层的一些不同的kernel，然后去造成的它的生成的一些结果的一些过程，然后更底层，然后这篇文章具体的分析，主要我 get 到点，就是说不同 batch size 或者不同 batch 会造成很多原因，因为我们在做计算的一些分布的时候，我们选择不同的 batch 或者不同的一些样本的时候，它整体在做推理的时候，它可能选择不同的一些kernel，然后导致它在计算的时候数值是不一致或者不可复现的。所以造成这个案例的结果。所以说整体归结最根本的话就是他计算的数值不一致，对。

说话人 2 15:38
这数值不一致，这我的理解其实就主要原因就这对你说的是非 thinking machine 他们博客上，对。

说话人 1 15:48
他们博客上详细分析了这个原因。

说话人 2 15:50
嗯嗯嗯，挺好的。然后就是多投资助力机制，你相关的这些操作你熟悉吗？比如说，嗯嗯，就是这个我相信你应该还是知道的，就是你能不能给我介绍一下，就是比如说我们则怎么得到多头的 QKV 矩阵？

说话人 1 16:17
嗯嗯嗯，好的，好的。比如说我们现在有一个维度为n，然后我们转化成我们现在原有的一个维度是n，然后我们转化成 8 个头，然后我们把这个 n 会去做一个维度的一个划分，然后划分成 8 个维，类似于去把原有一个大的维度信息，我们划分成不同，小的一些维度的一些信息不小的不同，小的维度类似于不同的一些样本，然后去做整个不同的头之间的一些交互，然后从而让它学习到更多一些丰富的一些信息。这是他做划分，就是。

说话人 2 16:50
他一开始的这个矩阵，比如说你刚才说512。

说话人 1 16:54
维，对对对啊。然后。

说话人 2 16:57
我们就假定一个序列是 n 个TOKEN，然后本来的 QKV 矩阵是几维的，你觉得。

说话人 1 17:08
你刚刚说假如说是 n 个TOKEN，然后它的维度是每一个 x 的话，它输入的 embedding 维度是 512 维，是吧？那么它原有的做线性变换，它的维度也是512。因为他会把。

说话人 2 17:26
对，然后这个就相当于我，我们其实是，嗯，底层是计算这些矩阵了。对对对，就QQ、 QKV 这些矩阵是几维的？维数是什么？

说话人 1 17:41
我没太 get 到。您的意思底层是。

说话人 2 17:46
就是它这个，我就我们想知道这个矩阵，嗯，它是几乘几的？就是QKV，就然后我们就要分发到每个头嘛？

说话人 1 17:56
嗯嗯嗯，好的，好。

说话人 2 17:58
哎，就是我想知道。是啊，比如说我们假定是一个序列，是 n 个TOKEN，嗯，好，然后我们就生成了一个一个线性变换，我们是生成了 QKV 矩阵，然后它的矩阵的维数是几乘几？

说话人 1 18:14
嗯，好的。比如说我们现在的是 n 个TOKEN，然后我们经过线性变换之后，它得到的长度还是n，然后乘以它的维度是512，比如说你说的是512，它的维度还是512。

说话人 2 18:28
诶，对对。你设定 512 嘛，对吧？对。然后那我们怎么我们有了一个矩阵 n 乘以512？我们怎么得到每个头的自助力投的这个 QK QKV？

说话人 1 18:46
我们会把这个类似于做一个，把这个矩阵去做一个拆分的一个操作，从它的维度层次去做一个拆分。比如说我们要去分成四个头，那我们就每个头的维度是 512 × 4，然后我们再得出另外一个类似于另外一个通道的信息，去得到一个4，然后去得到通过这样的拆分，其实它里面包含的信息是一样的，只不过它在计算过程中可能是需要每个维度或者每个头去关注不同的一些信息，从而去后面的一些计算。对。

说话人 2 19:17
然后接下来我们还要就得到了每个头的，嗯，然后最后还要进行什么操作？嗯，就是我们得到了每个头，然后训练嘛嗯？嗯，模型训练嘛嗯？然后完了之后，嗯，然后接下来怎么。

说话人 1 19:35
操作？接下来您想问的是那个自注意力机制是怎么做的吗？还是。

说话人 2 19:41
对。

说话人 1 19:42
注意力机制的话，它是把我们每一个类似都要多头技术有QKV、 q 和 k 的转置乘以做点。

说话人 2 19:49
抱歉抱歉，我表述的不太清楚，我表示不太清楚。最后是最后这些多投资注意机制的矩阵，嗯，我想知道他们是怎么拼接起来。

说话人 1 20:01
他们拼接的话是通过类似于 view 或者 permute 的一些操作去得到一些拼接，然后您想问的是他。

说话人 2 20:14
拼接拼接之后的维数是多少？

说话人 1 20:17
拼接的维度还是和原有的维度是一致的，原有输入是512，那么我们输出也是 512 维度。对。

说话人 2 20:26
他拼接完了之后，我们要怎么对他进行？

说话人 1 20:32
您说拼接就是把多头的注意力去拼接，完成之后怎么去进行操作吗？

说话人 2 20:38
对对对对对。

说话人 1 20:40
我们会把他的 QKV 去得类似于得到一个 attention 结果，然后我们去经过 softmax 的一个概率的值，然后去得到概率之后去做拼接，然后我们把这个拼接的一个结果直接就做输出了，或者说。

说话人 2 20:58
或者就是说我们能不能直接拼接？就是比如说拼接成，把它拼起来成，拼成一个 3 乘以512，然后就是输出了，可不可以。

说话人 1 21:12
实际上后面的好，还有一个类似于 Linear 层的一个操作，去把我们整个得到结果去做一些类似于信，多进一步把信息做一些信息的一个融合，然后得到一个输出。对。

说话人 2 21:25
就是你有没有想过就是你刚才提到了一个线性操作嘛？嗯，这里的线性操作能不能把它。

说话人 1 21:41
改成MLP？我理解是可以的，就说我们可以去增加多MLP，是多个线性层，嗯，对吧？我去增加一些多。

说话人 2 21:51
嗯，比如说我想在这个，嗯，行，好，然后我可以，嗯，就是引入非线性层，我可以引入非线性层去处理它吗？嗯，也是可以的，是吧？

说话人 1 22:10
这里应该也是可以，但是我们在做 Sigma 的时候，其实上是做激活函数的，实际上是已经引入了非线性层的一些操作，所以说我觉得在后面一步就可能没有必要再去做引入更多一些线性层。

说话人 2 22:25
诶，那比如说这个位置编码，就是我们还是刚才这个例子，好吧？就是你举的例子 512 位。就是比如说我要想要做旋转位置编码，旋转位置编码就是刚才这个矩阵 n 乘以 512 位，然后我要做旋转位置编码。嗯，那你觉得这些向量要怎么处理？

说话人 1 22:54
做旋转位置编码的话，它大概的一个操作是会把它的一些不同维度类似于以两个维度为一个一个组合，然后去加上一些不同的一些旋转矩阵，然后您说的关于它的维度信息可能就是要去把每两个去做一个维度的一个类似于匹配，然后去添加同一个类似于选择向量，然后去做相关一些操作。

说话人 2 23:23
对对，差不多就是每一个行向量，然后就要分成二二二二对对，然后乘一个对，然后那就是，好，我们现在就有，已经有了 QKV 矩阵了。嗯，那但我们目前的工作可能不涉及到一些模型的预训练，但是就是你知不知道他真正预训练的时候有，ok，我们现在有了 QKV 矩阵了，他就是怎么初始化，这个你知道吗？

说话人 1 23:53
对，我们现在有了 QQ 矩阵，是对 QQ 矩阵做初始化吗？对，我理解。

说话人 2 23:59
初始化，然后你然后进行训练嘛？

说话人 1 24:03
对吧？我理解。嗯，您问的就是说我现在有了这个模型，我现在对，可能要先对初始化，然后对我们的数据做训练，对吧？然后我理解这个。

说话人 2 24:13
我就是问这个模型的预训练，模型的预训练就是模型预训练的过程当中它肯定要初始化，然后计算一个loss，然后再 back propagation，对对对对对，那我觉得想问的是，就是你知不知道就是我们是不是可以随意地去，随机地去？那个叫什么初始化？这个可以吗？

说话人 1 24:34
这个我们是有一些初始化方法，我觉得是不能够去做一些随，很随意的初始化。

说话人 2 24:40
对对对，不是不能的吗？对，我的想，那我下一个问题就是说我能不能用标准正态分布去初始化它？因为标准正态分布它其实是一个分布，还是不错的，一个一个概率分布。那我就是，我如果想要用标准正态分布去初始化它可以，你觉得？

说话人 1 25:02
关于这个用标准正态分布去初始化的话，我觉得你要这么初始化也可以，但是现在大家做的可能或者说大家整体的一个数据的一个分布的话，可能它存在一些有些极大值，或者有些极小值的一些类似于包括我们做量化的时候，我们很多时候做量化的时候，它有一些，比如说 AWQ 量化，它是我们有一些存在偏差较大的一些值的，所以说我认为可能不适合做这种标准正态分布，因为我们可能还存在一些分布较差。或者说分布较为广泛的一些数据，所以说可能不是太适合用标准的正态分布，然后现在做的可能比如说用一些凯明的正态分布，凯明分布以及其他的一些大家在做的一些分布，可能不是在标准分布。

说话人 2 25:55
嗯，对，提到凯明这分布，就凯明分布它相比较这个标准正态分布它有什它的分？它长什么样了？它数据表达是长什么样？它跟相比较标准正态分布它有什么优势？

说话人 1 26:09
凯明正在分布的话，我说实话，我这个具体公式我应该是看过，但是具体的里面的一些详细的我可能不太已经忘记了。对，嗯。

说话人 2 26:19
嗯嗯，反正就是他就比他可以用来。

说话人 1 26:23
初始化。对对对，现在都可以。对，现在很多都是用的这种方式。哎，行。

说话人 2 26:30
对，我只是他们给我没啥可问的，我懂的，可能也不一定比你多，然后我们，然后这个岗位他可能相对来说还是比较，嗯，要写挺多代码的，然后我这边就，嗯，我们交流一些 Python 嘛。

说话人 1 26:50
好。

说话人 2 26:52
对。然后你平时写 Python 写的多吗？

说话人 1 26:55
平时写 Python 写的多就主要是用 Python 写，因为做算法这些东西啊。

说话人 2 26:59
对对。然后就是你你你这种异步编程你写的多吗？

说话人 1 27:06
异步编程的话也写过一些，主要是用一些，比如说我们 API 做 request 的时候，我们需要去做多多的，就是比如写一些线程池的方法去做一些相关的返回。

说话人 2 27:20
就是你你知不知道就是 Python 它的多线程就是照理来说它的底层是Cpython，c， Python c 是它可以任意地调用这些自然内核的，但是 Python 它没有办法做到。嗯，就你觉得是，为什么 Python 多线程为什么没有办法调用多核？

说话人 1 27:47
关于这个 Python 的多线程为什么没有办法调用多核的话，我看之前看的一个，我之前也看过一些相关的博客，好像是说的是它每一个程序都是采用一个单独的一个进程去操作，所以说他没有办法去做到一个类似调度，就是说一些相应的一些过程，这是我可能不太记得太清楚，就是了解到一些内容。对，嗯。

说话人 2 28:17
就是那为什么 c 可以， c 加 Python 不可以？

说话人 1 28:30
这个我可能没有太详细的一些做底层了解。

说话人 2 28:36
你平时写 Python 的时候有没有考虑过这样一个问题？就是判别的时候，就是比如说 is 跟等于它俩一样吗？就判别的时候，比如说一个东西 IS 或者是。

说话人 1 28:59
等于和什么。

说话人 2 29:01
等于等于和ISIS。

说话人 1 29:03
是吧？我它底层不是一样的，我理解它 is 的话和等于它一个是判断它的一个数值是否这样，一个是它的底层的，类似于更底层的一个位置，或者说放大的位置信息是否是一致的？是我理解的。

说话人 2 29:22
位置信息，就比如说我举个例子，嗯，就是 a 它是一个list，60，嗯，然后 b 也是一个60，嗯，一个list，嗯，那你觉得 a is b 跟 a 等于 b 这两个它分别会返回什么？

说话人 1 29:46
首先 a 等于 b 的话它是true，然后 is b 的话它是false，因为它们底层放的一个位置，或者说它我们在处理定义时候，分布定义的时候，它放置的一个类似于地址空间就是不一样的，所以说它返回就。

说话人 2 30:00
地址空间不一样的，它俩不是一个对象嘛。

说话人 1 30:02
对对对对对对。

说话人 2 30:04
所以说就是 Python 里面就是如果是直接赋值 a 等于 b 的话，它是不是有会有什么现象？就是如果直接赋值给 a 等于 b 的话，它会有什么现象？

说话人 1 30:20
如果赋值 a 等于 b 的话，它会，我理解它会把 b 的那个地址空间以及或者地那个地址去给 a 做一个赋值，就说 a 的这个地址现在被换成 b 的同一个指向同一个地址。

说话人 2 30:43
然后如果我修改 b 的话，是不是就会修改a？对对对，那如果我不想要修改 b 的时候修改a，我应该要怎么做？

说话人 1 31:03
嗯，他可以给他附两个值复，换成不同的一些地址或者是怎么样的？就说我他们两个是独立的一个地址，或者说独指向独立的一个单元。

说话人 2 31:24
Python 当中要怎么实现？

说话人 1 31:31
嗯，这个话我觉得我们可以独立设置两个变量，或者是两个对应的一个东西，然后去做相应的一个部，我们要比如说。

说话人 2 31:46
这个东西很复杂，它可能是一个非常复杂的class，然后我要用 a 来初始化b，但是 b 修改的时候我又不不不， a 修改的时候我又不会来改变b，那这个要怎么做？

说话人 1 32:03
嗯，这个话我可能没有太深入的一些了解。对。

说话人 2 32:12
这个没关系，我们就简单探讨一下，然后 Python 里面那种你有没有写过什么什么装饰器这种东西？即使装饰器这种。

说话人 1 32:35
用过，但是没有实际去写过一些相关的用过。对。

说话人 2 32:40
你觉得装饰器它有啥。

说话人 1 32:41
用？装饰器的话它主要是说我们直接用到这个装饰的话，它可以很快去做一个相应的一些调用，或者说把这个装饰器里面东西做一个相应的一些执行。对，包括我们有一些，那。

说话人 2 32:58
我们为什么就不重写一个函数。

说话人 1 33:03
装饰器在放在我们这个函数的一个入口位置的话，它能够起到一定的一些记忆作用，比如说我们写一个 cache 的一些装饰器的话，它能够去记忆我们当前的输入这个函数的一些相应的内容，然后去得到一个快速的一些返回。然后是我理解它装饰器可能更方便，或者说更便捷的去。

说话人 2 33:27
方便它的输入。输出是啥？

说话人 1 33:32
装饰器的一个输入的话，是我们调我们装饰的这个函数的一个输入。

说话人 2 33:38
那它输出是啥？

说话人 1 33:40
它输出的话是我们预定义，或者说我们预定义这个函数已经我们需要去完成的一些输出。

说话人 2 33:51
预定义完成的一些什么？

说话人 1 33:54
比如说我当前的一个函数要完成的一个相加的一个操作，然后我现在的是这个函数的输入是两个值，然后它能够返回两这两个值的相加结果。那么我们现在可以设置一个装饰器，比如说设一个开始的装饰器，它能够读到我们当前的输入的两个值，然后它的输出可以是返回我们当前的这个函数的一个输出的结果，从而去达到这个装饰的一个效果，就完成这个函数的功能。

说话人 2 34:25
然后你有没有写过那种 lambda 函数？

说话人 1 34:30
写过 lambda 函数。

说话人 2 34:32
就是你觉得 lambda 函数它有什么劣势呢？就是它写起来很简单，那你觉得它跟一般的函数相比， LAMBDA 函数它有什么劣势？

说话人 1 34:44
Lambda 函数相较一般的函数的话，它在里面没有办法做到一个很详细或者一个很复杂的一些定义，比如说我有多层的一些 if else 的话，我们可能。写得比较复杂，然后我们可以通过一些函数来实现，然后 LAMDA 可能就适合一些比较为简单的一些遍历的一些类似于一些操作。

说话人 2 35:10
但是它也可以写得很复杂，一些简，一些复杂的样式它也可以实现。

说话人 1 35:15
那我理解它可能在实现的过程中，我们可一方面相较于我直接写一个函数的话，它整体辨识度或者说我们方不方便我们二次做一些相应的一些更改，以及它在底层上可能 LAMDA 函数的话，它实现可能更可能和正常的函数它也有一些不同，这点我具体没有太了解。对，嗯。

说话人 2 35:42
我们就是做一道简单的题目吧。嗯，好，做一道简单的题目，我们你跟我说一下思路，可能就不用再写。

说话人 1 35:53
了。嗯，好的。

说话人 2 35:56
你就共享一下屏幕就行。好的，就是什么 ID 都可以。

说话人 1 36:01
嗯，好的，可以看你屏幕。

说话人 2 36:15
可以的，就是我题目发出来了，就是在我们的聊天记录里，我也口述一下。嗯，好，输入就是一个数组，输出就是连续子数组最大了。

说话人 1 36:40
嗯，好的好的。

说话人 2 36:43
然后你跟我说一下思路，就你想怎么做？

说话人 1 36:47
嗯，好的。然后这道题的话大概的一个做法是求连续最大子数组，那么我们可以写一个前缀，类似于一个 for 循环，然后去计算它当前到当前的一个 sum 的值。

说话人 1 37:05
for i in range。

说话人 2 37:10
按前缀和来做，对。

说话人 1 37:12
前缀和来做就可以了。然后去记录它当前的一个页和，然后记录它当前以及前面最小的一个 mini sum，然后把当前的一个 sum 去，然后维护一个全局的 answer 变量去计算它当前的一个 sum 减去 mini sum 的一个值，然后和当前的一个全局的 answer 的一个值的大小，然后去返回它的最大值。对。

说话人 2 37:34
嗯，再然后它的时间复杂度和空间复杂度。

说话人 1 37:38
是多。时间复杂度的话，因为我们需要去遍历一次就可以了，它的时间复杂度是on，然后它的一个空间复杂度的话，我们需要去维护几个，一个 current song，一个是 mini song，以及我们一个全局的 answer 变量，它的空间复杂度是O1。

说话人 2 37:54
对啊，这是这道题目就可以了。嗯，好。然后我想请问一下，你就是也是一道开放性的题目，嗯，就你可以说一下思路，然后写一些伪代码，嗯，也可以不写，嗯，好，但当然代码如果能运行最好，就不能运行，你就写一些思路就行。嗯，好，就是比如说我想做一件事情，就是比如说我的代码能够我的一个聊天机器人就可以提取一些用户的记忆，然后我可以长时间的存储这个长时这个记忆。然后就是就比如说我提我用户提到了就是我是某个地方的人，然后比较喜欢吃辣，嗯，然后这个记忆我希望就是长期的能够存储下来，然后你觉得就是你会想什么办法来实现这个带有记忆功能的这个聊天机器？

说话人 1 38:55
嗯，好的，首先我先大概先写一下思路，然后可以，嗯，首先有一个类似于对话的一些history，就是我历史的一些交互的一些信息，以及这个 history 是动态变化的。然后以及我们可以设置一些 memory 的一些机制，这个 memory 的话可以类似于我们人类的一个有 long term 和 short term 的一些相关的机制。

说话人 2 39:36
对，我希望比较关注这个 long term，你们怎么存这个 long term 呢？

说话人 1 39:43
long term 和 short term，然后比如说我当前的有一些信息，或者说我当前和我们的一些对话的一些机制，然后我可以去放到我们这个黑色羊绒去，然后我们可以去调用一个模型。然后去让他判断我们可以提前预定好哪些数据是 long term，哪些数据是我们长期需要交互的保护，长期交互需要去或者说哪些数据是对我们这个客人、这个用户本人的一些特点的一些信息，然后我们去把这个每一轮交互，我们都去调用这个agent，或者都调用这个类似于大模型去得到，去提取哪些数据是需要去长期保存的，哪些数据是可以忽略掉的？然后我们去，如果，比如说我们现在当前的黑色中有用户的一个描述是我喜欢吃辣的。

说话人 1 40:41
我喜欢吃什么东西之类的东西，然后我们会调用这个模型，当然这个我觉得这个 memory long term 的话也有很多详细的一些分类，或者说里面有些详细的设计，比如说生活习惯类的，或者说我的家人那些东西相关的，然后比如说我们这个爱好的话，我们可以放入一些爱好类的一些信息当中去做一个保存，然后去放在这个字典里当去，然后这里可能还涉及到很复杂，比如说我用户前几天说他喜欢吃辣的，然后用户这几天又说他不喜欢吃辣的。我们可能需要去做一个更复杂或者说更多上下文的针对这个 long term 的一些信息的一些保存，然后去做相应的一些 long term 的记忆的一些生成，然后这是我理解的我们怎么样去保存它的一些记忆信息，对。

说话人 2 41:27
然后这个记忆怎么检索？就是因为我们需要给一个回答问题嘛？嗯，然后这个记忆怎么检索？

说话人 1 41:35
嗯，好的啊。然后比如说我现在用户的一个输入是。

说话人 2 41:43
给我推荐一家餐馆。

说话人 1 41:44
对，推荐餐馆，嗯，那么这里的话可能就需要去调用我们的一些长期的一些记忆，然后去给出它的对应的用户的一些爱好一些推荐吧。然后我理解，首先我们需要去，我理解在这个 long term 里面的话，它可能有一些不同的种类的一些划分，比如说生活之类的，然后以及爱好之类的，以及什么家属的一些什么之类社会关系之类的这些东西。然后我们可以去根据我们这个用户的query，然后做一个意图的理解，或者说我们去划分到哪一类当中去，然后去做子类的一些相关的检索，或者说它的 long term 记忆的一些返回，然后去拼接到把这个用户的 query 加上我们这个 long term memory 这些机制去喂给我模型，然后去做相应的一些生成，然后从而达成个性化的推荐。

说话人 2 42:54
嗯，然后你其实刚才提到了一个很好的问题，就比如说用户之前说我喜欢吃辣，后来又说不喜欢吃辣，嗯，那我们要怎么更新这个？比如说就是他的饮食偏好这一个这个槽位，就我们要怎么更新这个槽位？

说话人 1 43:20
关于这个槽位可能就是说我理解这个问题，就是说我有时候用户可能存在一些信息的更新，或者说存在意图这些转换啊。我理解这个槽位的话，我们也可以去比如说我他的一个 d 式的一个类，比如说我们可以先是刚开始我们没有是空的，然后我现在，比如说现在有一个 lark 信息，然后我更新进来。然后后续他可能又说他喜欢吃，然后在这里我觉得可以去保存一些，其他除了这个辣的这个东西的话，可以去保存我当前说我这个喜欢吃辣的这个东西的一些上下文的一些信息，从而不仅仅去保存辣这一个东西，因为有时候用户可能存在多轮交互，或者说存在一些长期的一些过程，他的一些我们要分析他的一些饮食变化，以及这个辣，可能是我在冬天喜欢吃辣，以及或者说我在夏天不喜欢吃辣，这个过这过程可能存在一定的周期性。

说话人 1 44:20
所以说我觉得可能需要去保存一下我们的一些系统信息，比如当天是什么时间，当天是什么天气这些东西，以及我在更新我的一些状态，比如说我现在我不想吃辣，我要吃甜的东西，然后以及当前的一些状态信息，或者说我当前的一些历史对话的信息，从而去，然后当我们用户有一堆新的东西过来的时候，我们需要去根据他的当前的一个状况或者当前的信息去做一定的总结，或者说找到一定的对应的规律，然后完成这样一个推荐，因为我觉得说的是有。

说话人 2 44:54
道理的。对，然后我们能不能简单地实现一个最简单的逻辑？就是最简单的。就你刚才这个，然后你能不能简单地写一下你，你打算怎么实现这个？就你刚才说的这个。

说话人 1 45:07
功能？嗯，好的，好的，我大概写一下，可能写一下伪代码。

说话人 2 45:12
对，写下伪代码。我觉得我知道你大概说的意思，但是我想看一下你是怎么实现。

说话人 1 45:18
嗯嗯，好的好的。然后这里话是可能只写 long term，对吧嗯？对，那可能是一个类，然后类里有很多东西，我可以预定一些相关的一些类别，但是实际上我觉得这个东西可能会非常复杂，如果说我们在长期做过程中可能不会只是我们这个预定义的一些类别的一些东西，可能需要有一些动态的，跟这个 long term 的一个字典，有一些动态的更改的一些机制，然后先写一个简单的类似于静态的一个东西，嗯。

说话人 2 48:15
我想问一下这里的 memory LM，嗯，你要怎么去实现它？要做个什么操作？就是把，要把 query 怎么。

说话人 1 48:26
好的？我大概写一下我的代码。

说话人 2 48:29
嗯嗯，你这个不用写，这我理解你什么意思。嗯，对。然后你就写一个最重要的函数，你就相当于把它本身的记忆也传进去，对吧？然后让模型来做判断，是吧？

说话人 1 49:42
对，因为可能用，对，因为用户可能说有一些新的，因为我们 query 进来之后可能有些新的爱好或者变化之后，我们需要去对它做一些update、一些操作，以及要根据它历史的一个。

说话人 2 49:54
你相当于还是传进去的东西，就相当于是一些已经有的记忆，然后你就写一些提示词，然后让它来进行某些程度的更新替换，是吧？对对对对。然后你这里的 extract 你想怎么实现？

说话人 1 50:14
extract 的话，我想的话首先我可能去根据，因为我们如果长期以来的话，这里有可能会是个非常大或者说非常复杂的一个 long term memory 的话，可能会去做一些层级的一些划分，比如说我可以去，我去根据这个 query 去进来，我去分发到哪一个类似 long turn 的子的池子里面，然后在这个子池子里面去找一些相关的一些信息，然后去得到一些返回。这我打算送一个 chart 的实现方法。

说话人 2 50:50
你刚才不是说了，就是你这个 extract 你要先做一个意图识别嘛，是吧？你刚才不是说要先做个意图识别，然后再从这个是当他们某一个子类里面选出来对相关的。

说话人 1 51:03
信息，对吧？对。

说话人 2 51:05
那我的问题就是说，嗯，比如说某一个子类里面有很多信息，那我怎么返回某哪一条信息最重要？

说话人 1 51:18
比如说某一个子类有很多信息的话，我觉得这个话可能涉及到一些检索或者 RAG 的一些过程嘛。比如说我现在问的问题是我喜欢去哪个食堂？或者说喜欢吃哪种食物？然后关于实物这个类的话可能有太多的一些东西了，那么我可能或者说。

说话人 2 51:41
你是相当于先做一个判别它是实物，然后再做一个相似度检索，是吗？

说话人 1 51:48
我可能去会把这个query，会把这个用户的一些意图去做类似于 query 这些改写，或者说和我们因为我们这个 long term memory 里面有一些其实类似于可以对它做一些标签化的一些东西，然后去做一些相关的一些分发，然后去再进一步的精简，不是返回一个很多的一些东西的功能，这是我想做的。就说有一个分类。

说话人 2 52:10
就是当你是相当于把这个某一个东西用大模型做一下总结是否需要更新，然后做总结搞完了之后你就直接返回这一整个类，是吧？

说话人 1 52:24
对对对。

说话人 2 52:26
直接返回整个类，然后那行，这样是可以返回整个类，然后把整个类的东西直接输出给大模型。

说话人 1 52:36
是吧？对对对对。

说话人 2 52:38
行，好，我没有其他问题了，你还有你有什么问题想问我？

说话人 1 52:43
我想问一下我们公司具体做的业务是很涉及到一些底层，或者说很涉及到一些原理上，因为我聊，刚刚我们聊过，就是我感觉我们问的还是比较底层，或者说涉及到底层的一些原理和一些计算，这个。

说话人 2 52:56
底层的东西。很抱歉，只是那。这个我们的一些，我们目前并不涉及到一些模型的预训练，有模型的微调，这个涉及到，但是不涉及到那个。

说话人 1 53:15
模型预训练。嗯嗯嗯，了解了解。那我想问一下我们公司现在做的主要业务是围绕哪一些方面的一些东西？对，哎。

说话人 2 53:22
哎哎，简单的介绍一下，就是主要是关于 AIOS 相关的一些东西，相当于我们希望就是用 AI 来重构车机上面的车就是汽车机系统，那这个其实涉及到的面比较广，可以思考一下，就是它涉及到了就是人机交互的这样一个方式，那可能一些我们需要开发一些简单的 agent 来帮助我们实现一些功能，就我们的一个语音识别完了之后，就是这些 AI agent 可以帮助我们自动地完成一些操作嗯。比如说打开车窗点咖啡之类的，就是最简单的人机交互，它可能还涉及到一些，就是一些模型在车端的部署，因为就是可能你也知道，就是一些模型它可能是车厂，就希望就是它能够本地部署的，那我们就是需要做一些模型的微调，然后单测部署，嗯，然后实现一些就是数据的本地化，然后完成就是更可靠的这种。

说话人 1 54:31
操作。嗯嗯嗯，了解了解，其他没有什么问题啊。对，哎，好的。

说话人 2 54:36
然后对我也没有什么问题，那我们今天就面试就到这。

说话人 1 54:41
好好，谢谢您，拜拜。好，谢谢，好好，拜拜。

