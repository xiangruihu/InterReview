2025年5月22日 上午 10:01|1小时 1分钟 21秒

关键词:
相关性、语义、梯度、规则、符号、模块、底层、二分类、推理、语言模型、模型训练、语义空间、对话数据、样本训练、模型输出、对话模型、量化指标、业务场景

文字记录:
说话人 1 00:00 
是啊，理解就是用户的一些对话数据，然后去做相应的一个，最后去理解我们所设定的一些function，然后召回商供应的function。然后比如说对话数据的话，它可能就是更多地说是做一些数据生成的方面，做一些数据，比如说我要有些情绪的一些引导。然后我还有一些相关的，比如说我有一些记忆的一些功能，然后嗯，去了解用户的一些记忆的一些相关模块，可能，嗯，我觉得对话任务可能是更需要全局考虑一些东西，比如说我对我这个应用的个范围，应用的一些例子，我需要有什么引导，然后去更考虑业务上的一些能，然后方阵扣的话可能更考虑就是说我有的一些现有的一些功能，然后怎么样来去不同的方阵之间不产生一些冲突？然后单独去准确召回这个方阵，这是我对这个业务上的一些理解。对，嗯。

说话人 2 01:00 
那我们在构造 function call 数据的时候需要注意什么？嗯，有什么特别需要注意的吗。

说话人 1 01:07 
大概我之前做过一些 function call 的一些数据的话，大概首先是我们的数据量一定要去尽量去泛化，去模拟用户，尽可能去触，比如说我们要构造某个方案的时候去模拟用户，尽量口语，就说有些口语化的表达形式，它可能说是像我们文本的，就说我们正常就是这么规范化的一些表达，就怎样去模范，嗯，加些类似于加些噪声模拟去模拟用户的一些多种多样的表达形式，这是一个要注意的事项，不然的话我们创造很多有规则或者说很正常、看上去很正常的一些数据。

说话人 1 01:44 
大家可能会怎么说呃？可能会。嗯，导致在线下我模拟或者说我们测试的效果很好，然后线上的时候他有可能用户点踩率的话就会很高。然后OK，这是一个，然后还有话就说，嗯，我们之前做的是一些方案，上课的话，嗯，会基于一些，就比如说我们要生成一些关于播放音乐类的一些 function 的一些 query 描述，然后可能说我们在生成的时候会基于我们正常的一些导向生成很多是，比如说是中文相关的一些内容，然后可能些实际上用户会有一些英文的一些相关的这些实体的时候，可能方案获得召回能力不够，或者说槽位的一些抽取能力不够嗯。可能这些方面的话，可能说要根据一些 badcase 线上去做一些数据的增长改查、替换或者修补一些相关的一些工作，然后还有一些，还有一个我觉得。

说话人 2 02:40 
就是说，就是我是，我想问就是比如说具体的话，就比如说假设你现在构造完方训课数据，训练完模型嘛？那嗯，后端需要怎么去识别？就他要如何去识别？什么时候我要去调工具，什么时候去返回用户的，就或者模型的内容呢？

说话人 1 03:01 
嗯，大概的话，我们这个范围的话主要就是做 function code 一些。

说话人 2 03:12 
我理解，我假设举个很简单，对，很简单一个例子，比如说你已经训完了这个模型，嗯，然后你线上投入使用，然后用户这个时候去使用。比如说他假设你这个里面有一个 function 叫查天气，嗯，那有可能用户问今天天气怎么样，他会去调工具，对不对？但是如果他不问，这么问他就说你是谁？或者说是你好，就问了句你好，那么这个时候模型可能就不是去调工具嘛？他就是说，诶，也说你好或怎么样，对，那么在后端他是怎么去识别说那我什么时候要去调查天气？这个工具有什么时候去直接把模型返回的？你好去返回给用户。

说话人 1 03:53 
在后端我理解就是说，嗯，我们会对用户的一个一个 query 做一个多路的一个链路，就有一部分会传到我们的一个正常的，类似于你刚说那个问答一个链路，然后另外一个的话会进入我们房客申扣的一个模型当中去，他会去看我们的当前是否触发我的这个方式。如果说它会触发某个function，它会把 function 做一个返回，同时去调用我们内部的一些function，然后把相关的，比如说我查好天气的一些描述，把返回我们之前的一个链路的一个正常的一个大模型去做一个拼接，然后去做在那个模型上去做一些相关性的一些返回的一些能力。

说话人 1 04:35 
对，嗯，就是说我们有一个可能，就是线整体线上的话是有多链路一个配合，就是 function code，专门的 function code 一个模型，然后他会把方案库模型的一些结果返回到一个正常的一个对话模型当中去融合这些方案库的结果做返回。

说话人 2 04:57 
嗯，那你训练的模型。模模，你是，你有训练模型吗？

说话人 1 05:02 
对，我做过训模型。训练。OK.

说话人 2 05:04 
OK.你训练那个模型它的输出要用，是吗。

说话人 1 05:10 
呃？我们这个整个标签的输出的话，有一些标签的输出，我们主要做 SFT 的一些这种训练方式，然后它的输出的话，我们想要的是它的一个方程输出，就是一个函数的输出。然后当，嗯，中间的话，我们有也会对一些复杂数据的话，我们会有一些推理的一些类似于推理的一些数据的输出的一些内容。然后我们大概会有用一些特殊的符号去把间隔开，然后最后的话，我们最后一个正确的目标的话，大概就是说它的中间的一个突出的一个格式的一些内，就是说他最后的一个 function 的一个做后面的一个准确率的一个识别率的一个检验的一个过程。

说话人 2 05:55 
那比如说我，你现在要去检验这个模型，它 function code 这个调用的 recall 嘛？那你要，那你怎么去提取它function？就是它输出的这个 function 的这个内容？

说话人 1 06:12 
嗯，大概我们整体是这样的，大概说，比如说我们在用户有个 query 的时候，我们会把整个 query 给大模型，比如说用户要查找天气，那么他正确的话会返回一个，比如说一长的推理，一长段的一个推理数据，然后还有一段的这最后的方程数据，他这个方式是，比如说我们的一个 function 是 get visitor information，然后它的有一些 location 信息，然后有些 time 信息，我会把这些数据根据用户的 query 做抽取出来回这个数据当中去。同时如果说我们做测试时候，我们有真实的一个 GT 的一个方案审的数据，我们会把模型的输出和 GT 的这个做直接就做硬对比，就是说看他的嗯，方向是否正确，然后如果方向正确的话，我们会后续可能会检查他的一个好位的信息，得稍微有点，比如说天气的一些，比如说时间，然后比如说它的location，这个。

说话人 2 07:13 
是已经把 function 的内容提取出来了，对不对。

说话人 1 07:19 
呃？我没太理解您的，嗯。

说话人 2 07:21 
提取完之后才能做校验吗？我的意思就是说，比如说你模，因为模型输出就是一段文字，然后这段文字里面可能需要去鉴别它哪一块内容才是调工具的内容，那这段内容是怎么提取呃？从功能实现上来讲，嗯。

说话人 1 07:46 
这块的话我们是有，我们在数据生成的时候就是有一定有一个固定的模板，大概说比如说，嗯，我有一个用户query，小布帮我查找北京明天的天气，这是用户的一个query，那么我们在生成相应的一个 GT 的时候，它有一个数据，就 get weather information location，北京 time 是今天。然后同时它有，它就是有一个 tag 标签符号间隔看就是它的最后的一个 function 的一个结果，然后同时中间有一些推理的结果，然后我们也有一些推理的类似于一个符号去给它间隔开。嗯，中间说。

说话人 2 08:25 
那你的那个符号是什么啊？那个符号？是。

说话人 1 08:29 
啊，我们大概就说用一个嗯括号，就是嗯一些中括号，然后去类似于一些c，加些类似于加些 sync 的一些字段，然后去通过一些特殊字段的，就是我们在生成数据的时候就已经强制了这种格式。

说话人 2 08:46 
组合，这样对，所以 function 本身输出也会有特殊的符号去包裹它们。

说话人 1 08:53 
对，也会有特殊符号去包裹它，我们最后去把，对，把这些方式给提。

说话人 2 08:57 
出来，那后端基本上就是靠识别这些特殊符号去提取的。对，OK，你了解过这些特殊的符号是怎么加到模型训练当中去的吗？

说话人 1 09:10 
嗯，我了解过一些相应的一些过程，然后它大概就是说它通过加这些符号让模型去知道，我们大概就是引入一些特殊，这些特殊符号就是说是一个特殊的token，然后在训练过程中它知道，比如说我们有些最简最常用的就说一句话，结束的时候它会有end，就是这个 token 结束的一个特殊token，然后这种内容，然后它在训练过程中会模型会学习到，比如说这是结束 token 这句话就结束了，大概是这种。然后比如说我们加一些，比如说 sync 字段。然后我比如说一些刚刚的那种最后结果的一些字段的话，它也是让模型去理解这个字段的话，就是说我要输出是当。

说话人 1 10:00 
当中是输出它的 SYNC 的一些内容，然后字段的结尾的话，就是说 sync 内容结尾的，然后后续的话我要生成一些，比如说 function 的一些相关的一些内容，我理解就是说通过引入一些特殊的token，让模型去理解，我在某一部分去，嗯，输出某一部分的一些 function 进来。对。

说话人 2 10:22 
嗯，那如果比如说我想要加的那个特殊的符号不在模型的词表当中怎么办？

说话人 1 10:30 
嗯，这个的话就是说可以，可，就。

说话人 2 10:35 
举一个正，就举一个非常常见的一个场景啊。嗯，就比如说现在大家都可能会拿 r one 去做数据蒸馏。对，就比如说你要生成一些推理的中间过程，你可能会用 online 去你这里也写了构建思维链数据，对不对？然后 online 的思维链数据里面它有一个 sync 的一个特殊的个标志位，对对，对吧？假设你下游的一个就是你想要做 SFT 的那个模型，它本身的词表里面没有这个 sync 怎么办？

说话人 1 11:12 
这个的话首先的话我们可以最简单的一些方式的话，就是说我们通过一些强制的，通过一些 pump 的一些方式，就是让它输出时候强制去输出一些 sync 的字段，然后去强制去规定这些相应的一些格式的一些输出，然后当然这种方式的话可能就是说还是通过 pump 的形式，它可能不能从模型，就是说参数或者说深入程度去了解详情内容。然后可以考虑就是说添加，比如说我们再额外去添加一些特殊的token，让他去通过这些特殊的 token 的方式，然后去了解我们这些规则范式。就说额外加一些token。

说话人 2 11:58 
这些特殊的 token 是怎么配置的？你有了解过。

说话人 1 12:02 
这个？对特殊 token 怎么配置的我没有详细了解。

说话人 2 12:08 
嗯， tokenizer 你知道是干嘛用的。

说话人 1 12:11 
吗呃？这个我，嗯大概知道，就是说，嗯，它会把，就说它会有一个词表，然后大概说把我们一句长句画的内容，然后去分成不同的一些token，然后主要是根据他的一些词表的一些内容，然后换成对，就是每某一个词。比如说你好，今天吃饭了吗？你好，是一个token，然后今天也是个token，吃饭可能也是个token，然后妈的话也是个token，然后一些符号转化成相应的一个类似字典的一些表达形式。然后后续会把这个字典去做以高危的 embed 映射，去映射它的一些高危的一些向量的一些形式。

说话人 2 12:49 
对，OK，OK，OK，对，对。我刚刚那个问题就是说假设我想要加了一些特殊的这种 special token，它不在我的这个词表里面，那不在词表里的话，你就算加到 prompt 里面它也没有办法表示出来。对对，是这样的。对，那这种情况怎么办呢？

说话人 1 13:11 
嗯，这种情况的话就说从根源的话可能说去添加，比如他我要添加一个特殊的token，他可能没有的话，那最直接的那是添加特殊的可能，然后大概的话可能说去在我词表里添加一个，比如说我要加一个单元，就加一个特殊token，就添加一个位置去映射它的一个相应的一个点那个key。然后我们可能后续关于这个 key 的话，怎么让模型怎么转化成高维的embedding？嗯嗯，可能就是说后续做一些，把这个 key 转化高维embedding，然后把这个 embedding 能输入到模型当中去，让模型去认识这些高危的embedding，可能是模型训练的层次，可能要做一些相关的一些特殊处理的一些内容。对。

说话人 2 14:04 
然后有个开放的问题，就是对于一个模型来讲，它的词表是不是越大越好。

说话人 1 14:11 
呃？这个不是一定的，就说如果词表越大，那我们想一个极端的一个方式。你好，今天吃饭了吗？他把每一个词都作为一个token，都做一个 token 的话，它就不能够去理解它词与词间的一些距离，比如说你好，它两个词之间是一个是个强相关的一个句子，你把它一个个画成你是一个字，好一个字，它就实际上不能够表达词与词之间的一些特征的一些分布，或者说一些字，词之间的一些相关性的一些。所以我觉得它不是词表越大越好，可能说要找一个合适的一个范围去。

说话人 2 14:54 
词语，词与词的相关性我可以通过。模型来学习为什么要通过词表来去表述。

说话人 1 15:08 
词与词的相关性？可以通过模型来学习呃。我理解的话它是，比如说我们两句话之间的一些相关性，它不是说把两句话的每一个字都来去计算它的一个相关性的一些内容，这样的话可能一方面可能会带来一些计算量，比如说会增加很多，然后二方面的话它两个字之间的一些相关的，比如说语义之间的一些相关的一些关联程度，它可能不会捕捉得很好。然后这是计算量的一些方面的一些内容。然后第二的话，我理解我们人类在感知或者说构建语言这个模型的时候，它本来就不是以一个字一个字的为一个单位，比如说我们英文的时候，它也不会是以 26 个字母，它也是以 26 个字母去做不同规则的一些拼接，去构建某个词语的一些意思，同时有些词缀或者词根的一些来表达一些相关的意思，它不是说通过一个个词的一些相关意思。

说话人 1 16:17 
那么我们通过 tokenizer 把词就说很强相关的一些词作为合并成一个 token 的话，我理解也是去模拟人类构建词语的时候的一种思维范式，去构建相关的一些内容，而不是说一个字，就是一个字的 token 的一些相关的内容，这是我的一些理解。对。

说话人 2 16:41 
你刚刚提到说把相关的字合并成一个token，这个操作是怎么实现的？

说话人 1 16:47 
嗯，这个话比主要说有些 BPE 或者BBPE。然后嗯，大概的思想就是说我们有很多，比如说我有一本书的一些语料，或者说我有一个大的语料库，然后它的语料比如说你好，你和好这两个字经常出现在一起的话，它会把这两个字合并成一个字，然后你好，然后后面有个逗号，今天吃饭了吗？然后比如说你好，今天他俩，嗯，我现在把你好两个字合成一个一个字了。

说话人 1 17:16 
如果今天的话，他后面话经常，你好，今天吃饭了吗？然后你们先忽略这个逗号，好吧？然后比如说你好，今天他又是经常出现，那么我会把你好，今天又合并成一个完整的一些token，然后通过这种把小的合并成多大的这种方式达到一个目标词表的大小，这是 BPE 的一个思想， BPE 合并词表一个思想，它大概就说把一个个小的token，比如说一个个单词，然后合并成一些中间的，就跟词缀的一些经常去合并的一些方法，达到一个目标词表大小，然后还有一些其他一些方法，然后嗯，还有一个我忘了写叫什么名字。然后他大概就说嗯，把一个比较大的词表，然后去合并成，把大的词表做一些，拆分成小的一些词，然后计算它的，把这个小的一些词表构建语言模型的一些，让他用一些小的词表来建，对语言做建模，然后去看他的一些损失，对语言建模一些损失，然后去构建他的一些小的一些词表的一些方式。大概我了解一这些方式，大概BP、BBP、 world peace 的些思想的一些内容，对，OK。

说话人 2 18:45 
我看到里面你说有一个就是提升这个语义空间覆盖能力和 query 泛化性，这个语义空间覆盖能力是怎么去衡量。

说话人 1 18:58 
我们这个话？嗯，云空间覆盖能力大概就是说，比如说我有一个问题，让模型去生成一个答案的时候，如果我通过反复去问同样一个问题，同样的问题， prompt 的形式可能是一样的。那么它生成答案可能即使我们调一些温度系数的话，它采样就在它生成的一些范围内做些采样，它可能也是那么几个答案，那么它的语义空间的话，你命中的语义空间可能就会比较小。

说话人 1 19:27 
那么我们想要再生成一些，就这个项目是做语料，然后我们想要去命中语义更多广泛空间，那么我们就，我就想一些方法，就比如说去提高我们这个 query 的一些表达形式，然后大概就是通过比如说同样一个问题，我们换更多样的一些表达，然后去命中语义它的一些词语之词，词语词之间一些相似度，让它产生更多泛化性的一结果，这是我们的一个大概的一个思想，对。

说话人 2 19:59 
他有什么量。量化的指标就怎么去衡量的？

说话人 1 20:03 
量化指标大概是我们有一个词语之间的一个相似度，然后还有一个词语间这些距离的一个指标。然后大概的思想就是说，比如说两句query，一个是今天你吃饭，一个比如说查天气类的，比如说首都气温如何，嗯，还有一个北京天气怎么样，然后这两句话可能说表达意思是一样的，但是它的表达形式是不同的，那我们会计算一个语义层次的一个in，嗯，把它转化为高embedding，然后计算它的语义 embedding 的一个相似度，那么是它的一个语义相似度啊。

说话人 1 20:39 
第二的话我们会直接把这两个词转化成一个token，转化成to，两个句子转化 token 形式，然后去查看它们两个 token 之间的一个编辑距离，然后去看它的一个编辑距离的一个嗯，长度。就说如果把词 a 换成词，完全变成词b，它需要编辑多少步？然后如果它的编辑步数越多的话，那我认为他的他词与 a 中的表达形式和词与 b 中表达形式是完全不同的，那么认为它的表达形式就是多样的，然后就做一个替换，对，OK。

说话人 2 21:16 
嗯， function call 的话你们内部去平衡，就衡量它的这个。嗯，准确率是内部的指标，内部的测试集是吗？

说话人 1 21:26 
对，我们用内部，因为我们这些 function 的话，嗯，都是我们自己定义的，我们只能用我们业务场景的一些测试。对。

说话人 2 21:35 
嗯，我们内部做的有去试过，就是拿自己的模型试过一些外部开源的测试集。

说话人 1 21:42 
嗯，这个的话我们具体没有做详细的测试，主要还是在做自己的一些相关的一些防审 code 上面做一些相关的。

说话人 2 21:55 
然后你是后面说，有说那个对 badcase 做分析，然后通过什么规则补充高奖数据，这个分析是怎么做的。

说话人 1 22:06 
呃？大概的分析的话，首先我们会查看一下线上，比如说有些用户的query，可能我们模型没有命中，我们大概看一下它的，嗯，哪些范围是没有命中？是因为我们数据没有造，这些数据的话模型不理解，还是因为其他的一些相关的一些内容？然后我们大概是这样看一些相关内容。然后其次的话，我们针对这些badcase，比如说刚刚说的，比如说用户可能经常会，可能会有一些英文的一些query，然后我们识别成中文，或者说我们对英文的一些歌曲或者是一些名，比如说我们有些软件的名称可能也没有提取好，然后我们会针对这些内容做些规则化提取，比如说我们生成了很多中文的一些歌曲的一些相关的性能，那么我们会在查找一些，比如说在语料，在语库中找一些英文的歌曲的一些名称，把中文的一些歌曲做一些规则化的一些替换。然后就这部分的话，可能就没有用模型来做相关的一些数据生成，因为我们已经有很多中文的这个的话就直接做一些规则替换，然后主要就经过基于这种规则的方法来做相关的一些 badcase 的一些修补吧。

说话人 2 23:20 
生成的数据怎么去衡量它的质量啊？就比如说像翻译或者说英文转中文这种，怎么去看他宣传的内容对不对呢？

说话人 1 23:32 
嗯，这个话我们这个场景没有做相应的，比如说翻译或者英文转中文的一些相关内容，我们整体是做方式 call 的话，还是看模型用户的一个query，然后他调用这个方式是不是准确的？主要做这个OK，对，可能没有这方面，嗯。

说话人 2 23:56 
然后我再问一些关于模型训练方面，就是，嗯，比如说你们 SFT 的话，模型是基于什么模型去做的？

说话人 1 24:04 
我们，嗯，基本上基都基于千问的 2.5 系列，做的一些是c，做一些相关性。对。

说话人 2 24:11 
OK，那窗口长度多大呢。

说话人 1 24:14 
呃？窗口长度我大。嗯， 358 号是 400 左右，三四百左右的一个窗口长度，因为我们用户对话的一些数据，它不会有太长的一个，太长用户也不会跟你对话时候输入很多一些相关性，对。

说话人 2 24:31 
你说的三四百是你实际数据的长度。

说话人 1 24:35 
对，我们实际的 tokenizer 的长度，不是，就是我转换成 token 那个。

说话人 2 24:39 
长度。OK，那模型的窗口训练窗口。

说话人 1 24:43 
模型它千问二的话它已经好几，我记得我是两 k 还是 3K 的， 2.5 的话基本上两三 k 的一个长度。

说话人 2 24:55 
那你们 ROI 蒸馏的话数据窗口是多大？

说话人 1 25:00 
我们蒸馏出来的生成的一些数据，大概也是大概一些短对话，大概是可能是两三百个 token 的一个长度的一些数据。

说话人 2 25:15 
那假设是都是两三百，然后你说千，你千万是用什么两 k 或者四 k 的窗口，对吧？那这样的话会有很多的空间其实是没有被占用的。

说话人 1 25:35 
嗯，对。

说话人 2 25:37 
那这样的话训练的一效率就会比较低，我们会怎么去提升这样的样本训练效率。

说话人 1 25:48 
呃？关于您这个问题的话，一个是它原有的一个能力，可能就是说 3 4K 的一个可以为它一个 3 4K 的一个托管。但是我们的话数据可能就给他一个两三百的token，关于您这个这个的话我是我的理解的话嗯。 3 4K 的它设计出来的，比如说它通过一些 rope 的一些旋转位置编码，把它原有的一个 embe position embedding 的一些长长度外嵌能力拓展到 3 4K。我理解它是可以最长极限的话，可以在这个三四 k 的范围内去做一些相关内容，它不一定说我的所有场景的话都是适合这些相关性，就说如果我们的语调给它训练的时候，它是一些短距离，或者说一些，嗯，相对较短一些 query 的话，嗯，只要我们比较数量是足够多，让它去它，我理解是它也可以去，很好去捕捉它的一些短距离的一些相关的一些能力。可能说，嗯，就说。

说话人 2 26:56 
不是，不就不，这也不是说它短距离的相关性是，就是一条数据，比如说你本身只有 200 个token，假设，嗯，那它进入到模型当中去以后，它会变成多长？你了解过。

说话人 1 27:23 
嗯，这个的话我没有详细地了解，然后大概的话他不是大概，我理解它，比如说它最长支取两 k 个token，它不是说把我们的一个序列全部转化成，比如说它最长支持两 k 个，比如说我给它输入 10 个或 20 个token，它不是说转化成固定长度，然后输入到模型当中去做相应的一个输出后，他大概的一个做法就是说他是把我们的 query 转化成 token 之后，比如说我们 token 这 10 个或者 20 个。他就会把这些 10 个 token 做一些 Transformer 的 QCV 相关的一些计算，然后转化为高维的一个 embedding 那些表达形式，然后去做后续的一个 encoder 的一个过程。 encoder 就是说去转化它的一个嗯， next token 生成它的一些相关后续的一个语调。就说我想表达是。

说话人 2 28:15 
他不是说那我们就跟顺着你的，是你的刚刚讲的说讲，接下讲下去嘛？就是你说到Transformer，那我们就回顾一下 transform 的这个结构能不能具体讲一下从输入一直到里面是怎么样子？

说话人 1 28:34 
嗯，好的。然后 Transformer 的结构的话，它大概就说输入是我们的一个个token，然后输入 token 之后它会转化成一个embedding，然后去把 embedding 之后，由于我们 transformer 的一个 attention 是 q 乘 k 的一个转置，它们是，它们之间是没有一个不会感知它的位置关系的，需要加一个 position embedding 的一些相关的操作，然后去把每一个 token 单独加一个相应的位置编码，然后会转化成传入到它的一个 attention 层。

说话人 1 29:11 
attention 层就大概说会把，就说每个 token 会通过三个矩阵生成q、 KV 三个矩阵。然后 queue 这三个人的话，他会把当前的 token 的 queue 和其他 token 的一个 k 做相应的一个点乘，然后得到它的一个相信相，把当前的 token 和其他的 token 的一个相关性做一个计算，然后会除以它的一个维度。

说话人 1 29:38 
然后这个数据维度的话主要说因为他做矩阵点乘的时候，他会把他的值的一个范围扩散到很大的一个，要么是偏大，要么偏小。然后他做 supermap 的时候，他会导致它的一个速度慢的结果，就是说会偏大或者偏小，不会去在一个，因为我在模型。训练的时候需要它的均值为 0 方差是一，它可能会更好，你说主要就是说做这个线性的一个放缩的一个作用，然后去做线性放缩，然后会把它的一个，把当前 q 和其他的 q key 做attention，之后的那个值会乘以它的当前一个v，得到它的当前token。在整个全局句子的一个相关的一个注意力的权重的一个结果，然后这是他的一个attention，之后这是他的 attention 的操作，然后 attention 之后。

说话人 2 30:31 
不好意思，我打断一下，就是你说 q 乘以 k 之后，就 q 和 k 点乘之后直接乘以v。

说话人 1 30:39 
吗啊？它会做那个Softmax，一个激活。

说话人 2 30:43 
o，OK，为什么要做Softmax？

说话人 1 30:48 
主要的话因为 q 和 k 在做完顶层之后，它的一个数据范围的话，可能就是说它会比较大，然后主要的话激活把它在就会在一个零一的一个一个空间内，然后去方便后续模型的一个收敛一些操作，主要是做方便模型的一些收敛。

说话人 2 31:10 
你是说做 Softmax 可以方便它收敛，对，OK，然后你之前讲说 q 乘以 k 是去计算它的相似度。

说话人 1 31:24 
关联程度，就说当前 q 和其他的一些当前 token 和其他的一些 token 的一些关联程度。

说话人 2 31:31 
对，那我们就回，就又回到之前讲的一个问题，就是比如说词表越大是不是越好？我说然后你说他们之间的相关性就没办法很好表示。然后我之前提一个想法，就是说那这些，比如说你好，你说你和好的之间的相似性，那不就可以通过 k 乘以 q 来学习。

说话人 1 32:02 
他是可以通过这种学习方式来学习的。我想表达是比如说你好和哈喽的话，他可能就说我们把每个词的话都转化成一个单独的，哼，他可能，嗯，整个词表的量就会非常大，然后他可能就说他学习的一个消耗的话，可能就会，就是消耗，消耗成本会增加。然后他写因为他要学的东西可能更多了，然后他的一个这个计算成本、学习难度的话，他可能会呈指数级的一些增加的一些相关的一些内容。然后还有一个，嗯，我理解的话，它把你好转化成一个 token 的话，它更符合，就是对，建模，对，就是我们对语言建模说嗯的一些啊。

说话人 2 32:57 
没事，我觉得我理解你，我其实就是从计算效率来讲这个是合理的，但是从相关性上来讲是有点问题的，因为相关性可以从 Transformer 本身去学习。

说话人 1 33:12 
对对对。

说话人 2 33:15 
然后前面有一个有一提到一个点，就是你比如说你进来一个，进来一条数据，假设你是 200 个token，嗯。然后你说会做一个 embedding 操作，对不对？然后那比如说像千问这样模型，假设它的窗口训练窗口是 4K 的话，嗯，那这个 4K 是体现在哪里呢？

说话人 1 33:46 
它的这个 4K 的话，它体现在它最常，就是我们最常输入的一些prompt，它可以支持 4K 的token。

说话人 2 33:55 
最长支持。那我如果我没到怎么办。

说话人 1 33:58 
没到的话没他话他会通过相关的一些 padding 的一些操作去填充这些。

说话人 2 34:11 
对， padding 是可以的，就是去填 0 或者填一些复数什么的。嗯，对，我觉得这个是，OK，我再看一下，对，我觉得 transition 这也问得差不多了。对于其实还有一个，我看你简历时候有一个地方是一，嗯，就是有点疑惑的，就是比如说你在做那个 Echo 上面那个比赛时候，你说你们是对这个拉玛 70B 和千问去做了个改造，对不对？对对。为什么要。把它改成 classification model，这个就是我说，我的意思说他们自己不能做分类吗。

说话人 1 35:06 
这块我们是考虑的这样一个维度的，因为我们如果说我们把这个改成 classic model 的话，主要就是基于两个原有的话，它的词表它输出的可能就是说词表大小的一个维度，然后我们，嗯，后续的话转化成它的一个 classic model 的话，可能就说把它嗯输入的维度改成一个二分类的一个维度，然后它可能就说我们先从模型的角度上直接就陷入它的一个突出范围的，可能就说表现的效果更好。然后另外的话就说他输出的话，我们可能就是说通过一些 pump 的激发的一些形式去让他去做相应的一些输出。然后我们我理解的话，通过就是通过直接对模型做限制，让它输出就只有是这两个通过泡沫的激发的话，可能嗯。那 power plan 可能还是需要把我们当前的一些语句转化成一些模型，底层的一些他对我们这个 query 的一些理解的一些相关的一些内容，做相关的一些内容。我理解。

说话人 2 36:16 
我的意思是你语言模型它本身，你比如说你让它输出的时候只输出一，或者0，或者只输出是或者否，它这个它不能做到吗？

说话人 1 36:28 
他能做到。但是我们关于您这个问题的话就是说，嗯，我们之前也做过一些相关的性能，比如说我当之前做相关的一些 query 新生成的时候，我直接让他生成一些 JSON 格式的数据，不要生成任何是格式的废话，它可能 10 条类，它可能会生成 8 条，可能是符合的其他话，可能有些其他乱七八糟的一些输出我通过，嗯，比如说我通过这种改完分类的模型，我直接就从根源上去不让他限制有其他的一些输出，直接就是让他输出当前的一输，一些输出概率，然后去做相关的一些分拆，可能会减小一些相关的一些误差。然后去，嗯，对，嗯，然后还有一个话就说，嗯直接输出的话可能也会基于我的一些采样，因为我们最后是生成的一些词表，一些概率，它会做相关的一些词表采样，做一些相关的生成。然后可能也会对后面一些结果采样，一些策略，对后面一些结果有一些影响。然后我们直接改成这种分类的一些 model 的话，它可能直接就输出结果了，不用就说对它影响的一些因素可能就变得更少了，就更聚焦于模型本身。

说话人 2 37:39 
但是你们前一种方案有试过吗。

说话人 1 37:42 
前一种方案我们试过一下，试，我们试过了，它可能就是效果没有改成，我们大概就是说是先是在通过 palm 的形式让它去选择，比如说 a 好或者 b 好，然后选择最后生成这一个 a 或者b，然后可能会有一些触发错误，就说他可能就是大概说，嗯，我们生成 10 个答案里，他可能，嗯八九个队，可能有，会有一两个组的话，他也能会升为a，但可能有些其他的一些内容输出，那么我们可能就要基于一些规则的一些方法去提出相关的一些内容。在它的准确率的话，如果它的准确率最后也是没有我们改成 classification 的一些 model 的效果好，所以我们最后就全部用成把这个模型的输出结构做了修改或做成这种形式。对。

说话人 2 38:28 
然后训练完之后因为线上资源然后测，又选了更小的模型做知识蒸馏。那为什么不直接对这个小的模型做训练？

说话人 1 38:41 
嗯，关于这个问题的话，我们当时也是做小做，对小的模型做了一些像，嗯，我们直接就说把我这个数据对小模型做训练，我们发现它的效果是不如大的一个嗯。嗯，我们用的 70B 和 72B 的一个模型做训练出来的一个效果。好的，那么我们考虑到可能说这个小模型本来就是它的一些，嗯，结构或者说参数一些原因不能够做，对这个问题做很好的一些理解，即使我们加大一些数据量，或者说怎么样的一些形式，它也不能够有很好输出。然后我们想的是说可以把它的一个 70B 和 72B 的模型的一个大的一个对知识的一些理解能力，把它后面输出的一个 logic 输出，对我们的一个小模型做知识蒸馏，让，嗯，同样一个query，让我们小模型的一个逻辑输出和我们大模型一个逻辑输出的一个，嗯，尽量去表达的是形式是一致的，去蒸馏大模型的一些能力，去做相关的最后的一些结果输出。

说话人 2 39:42 
这个知识蒸馏是怎么做的。

说话人 1 39:45 
知识蒸馏大概的话我们就是说我们会把同一个 query 同时输入到嗯，70B、 72B 的这个模型上去，也会输到 909 B 的这个量化模型当中去，会得到它最后的一个 Logic 输出，然后把 Logic 输出做一个对齐，大概就说我们用的是二维交叉商损失去让它去计算一个它们输出的一个损失，让这个小的模型的损失尽量去靠近它们两个大模型的，让小模型的多久输出。尽量去靠近它的大模型的多久输出，做一个 loss 的限制，然后去蒸馏这个。嗯，小的模型。

说话人 2 40:32 
那这种方式跟我直接把大模大的模型的输入和输出做成那种样本，对小模型做微调有什么区别？

说话人 1 40:52 
我理解的话，您的意思是说，嗯，直接把大模型和小，把大模型的就说直接把我还，我理解。你那问题还是说就我直接把数据给小模型做样本的学习吗？我可以理解这样，对，这样的话可能是。

说话人 2 41:08 
你的 loss 也是。

说话人 2 41:09 
Sure. Yes, cross entropy loss.

说话人 1 41:14 
对，嗯，那这样的话，我理解的话，嗯。小的模型它可能直接就是说的是我们学到，就是说我们，嗯，最后 01 的一个一个概率，就是说我们是01，就是有一个 in 的一个结果，然后我们通过学习它蒸馏的一个小两个大模型，蒸馏了一个最后的逻辑的话，可能说他这样学习的反表达形式可能会更好一些，就说他不是一个强的 0 到 1 之间零一，然后他可能说是，嗯，他的表达空间可能会更大一些，大概是这种。

说话人 2 41:58 
非 0 和非一的这个数据它怎么做？ cross entropy，嗯，就是一段连续的，你的 Logic 是连续的。

说话人 1 42:10 
对，我们这块的话我们用了一些那个呃。我们这个用的是交叉熵损失，然后我们那个对大的模型做蒸馏的是用的是二项交叉熵损失二，分类的较少损失二，他有。

说话人 2 42:32 
蒸馏的时候是二分类。

说话人 1 42:34 
对，不是蒸馏的时候不是二分类，蒸馏是，就是一个交叉商损失，然后那个我们需要的话就是说让小模型的输出和我们的那个小模型的输出和大模型的输出的 logic 对齐。然后我们做大模型训练的时候是交叉熵二维的一个交叉熵损失，就是二分类的一个交叉熵损失。

说话人 2 43:02 
这个本质是想让小模型学的东输出的内容跟大模型很相似。

说话人 1 43:06 
是不是？对，这个本质是让小模型去输出，结果和大模型是一致。

说话人 2 43:11 
嗯，那为什么不用开聊散度呢？

说话人 1 43:15 
嗯，没有care。散度的话我们当时也有试过，可能效果没有，就说我们当时做了一些小批量的测试，可能效果没有好，我们就直接就用这个加上损失。

说话人 2 43:38 
好，那我就是我的文字部分差不多就面到这，然后我们可能最后还有一个代码的一个测试。

说话人 1 43:48 
嗯，好的。

说话人 2 43:50 
然后因为是这样，就是这个腾讯会议它没有那种交互式的写代码的，所以我一般就是把一些代码题目贴到这个聊天框里面。嗯，好。哎，我们的代码测试也比较简单，都是一些实操的。然后我把题目写出来之后，你就分共享一下屏幕，写一下就行。嗯，好，好。这个题能看到吗？看到。嗯，我打开编辑器。

说话人 2 45:07 
你如果准备好了，你就共享一下屏幕。

说话人 1 45:09 
吧。嗯，好，我打开编辑器。

说话人 2 45:56 
嗯，你这边方便共享下屏幕吗？可以看见吗？

说话人 2 46:08 
这个是这样，就是你不用写得特别的具体就是这个 module 能够 run 起来，不需要你写个大概的意思就行，包括比如说你有时候 touch 的一些模块，你名字记不得了，什么都没关系。

说话人 1 46:23 
好的好的。

说话人 2 46:40 
名字的话随便起了。嗯，我们就大概看一下功能就行。

说话人 1 47:56 
嗯，大概是这样的，可以吗？

说话人 2 47:58 
嗯，可以，这没关系啊。对，基本上就只要写一个这个就可以，然后有几个问题是一个是这里为什么要写个 super 呢？

说话人 1 48:12 
super 的话主要的话它需要继承 ND module 的一些相关的一些波泰的一些相关的内容嗯。就说他刚刚点 module 的话，他对里面有些底层的一些相关的一些内容，他要不要继承过来？

说话人 2 48:32 
就继承他的父类，是吗？对对对，OK，OK。我觉得这个 module 写得还是可以的。然后这个 module 有一个 follow up 的一个问题，就是说它在，它是它前向运行的时候是正常的，但是反向的时候可能会出现none。然后这个要请你分析一下为什么？我稍微写一下？

说话人 2 49:18 
就是你发现比如说假设你这个 model 已经写完了，我们也投入模型训练了，嗯，然后检查 log 的时候。后出现了一些烂的情况。

说话人 1 49:32 
我大概，嗯的，明白。

说话人 1 49:57 
嗯，我大概。理解的时候就是说如果 x 值和 y 值的话，它的值的话都很小的话，经过 sigmoid 之后，它的值其实也会很小，接近于 0 的一个状态。然后我们再进行他做相乘开增的话，可能会就说他的一个嗯。梯度就消失了，因为我们知道 signal 函数它对，它是一个 s 型的，就是说如果说它的值非常小的话，它可能会产生梯度消失这些现象，主要的话可能是什。

说话人 2 50:43 
什么值什么值非常小。

说话人 1 50:45 
嗯，我们 x 和 y 值如果是非常小的话，如果之后再做 Sigma 的话，它的一个曲线导致它的梯度会非常，最后 SIG Sigma 之后的值会非常小，然后它的一个梯度的话也是非常小的，然后最后可能会导致。

说话人 2 51:07 
梯度消失。

说话人 1 51:10 
对，然后我理解它产生那样的原因就是梯度消失了，没有梯度了，然后后续反正传播传不过来了，所以就会产生这些原因。

说话人 2 51:22 
我你往梯度就是梯度，往就是往取值范围这块考虑是接近了，但是表述上可能会有稍有一点问题。就是 Sigma 的一个值它在什么样的情况下会变接近于，比如说变得非常的小，你就说你说 y 非常小的情况下，非常小是指什么样的一个范围？

说话人 1 52:05 
嗯，这个具体值的一个范围的话，我可能就是没有太过于关注，但是，嗯，可能一的十，十的- 8- 9 次方，这个就我没有太关注，不好意思。对，这个方具体范围的话。

说话人 2 52:24 
假设。

说话人 1 52:28 
y 等于 0 的时候， Sigma 也是多少， y 等于 0 的时候， Sigma y 等于 0 吗嗯？就比如你的意思是 c 经过 c 后模后的结果等于 0 还是输入的输入是 0 嘛？树林 Sigma 好像是等于一还是嗯什么一个范围，反正是在 0 轴之上，嗯的一个。

说话人 2 53:05 
对。那当y，比如说接近于负无穷的时候，Sigma。

说话人 1 53:11 
y 等于接近于负无穷的话，它因为它的 x 可能接近于。

说话人 2 53:28 
当 x 和 y 接近于负无穷的时候， sigmoid x 或者 sigmoid y 就会接近0。

说话人 1 53:39 
对。

说话人 2 53:41 
那我们在求开根的时候，求它的导数的时候，它会怎么样？

说话人 1 53:50 
嗯，求他导数，还有他的会进一步去放把他的一个结果，因为我们知道就是说做导数计算的时候，如果去求他的就说这个导数运算，然后再去求导的话，它会进一步去放缩它的那个，就进一步会去缩小它的一个结果。比如说根号 x 导数，就是。

说话人 2 54:20 
对，这里可以到时候再去推一推，反正现在的结论我可以先告诉你，就是求导以后会出现 0 ÷ 0 的现象，导致它的梯度变成none。

说话人 1 54:34 
好的，了解，我回去再推一推。

说话人 2 54:37 
然后就更进一步的一个问题是假设我们已经观察到这个问题了，那么嗯，如何去解决。

说话人 1 54:47 
这个问题的话，如果林淑玲的话，我们可以把分母做一个，就加一个小，很小的一个常数，就加一个很小的。比如说一的- 10 次方，一的- 11 次方的一个常数项。

说话人 2 55:04 
这个是比不是？就是比如说让你修改这个 module 是怎么改？

说话人 1 56:13 
嗯，我理解，可以就说加一个这个值的，乘一个很小的一个常数，因为他最后求导的时候可能就加的是这个常数，然后导致这个方面就说分子分母的话可能就是说不是0，出现 0 的一些现象。嗯，可能不是直接相加的一些形式，就说在分母上做一个对 x 值乘一个相应的一个很小的。

说话人 2 56:44 
分子。有分子和分母是已经出现在求导的过程当中的，你现在加的是在前项当中去加，那求导对，其实它是不影响。

说话人 1 56:58 
对。

说话人 2 57:29 
那没关系，反正就是我可以给一些提示，你可以面试之后可以有兴趣话可以再探索一好，因为你说了就是。嗯，因为我们现在已经知道他会求导时候会出现 0 除0，所以你想避免这个问题，嗯，就要在你说 0 除 0 的时候去添加一些很小的值，避免这个情况，那其实也可能就会涉及到要你自己去手写它的反向传播，因为你现在继承了它这个module，那就是用它的自动反向传播，对不对？所以如果你想要改的话，你得自己去写一个这个 backprop 的一个公式，在这个 module 里面。

说话人 1 58:15 
嗯，了解了解啊。

说话人 2 58:17 
来了解。对，我们这个代码面试就先这样吧。

说话人 1 58:23 
嗯，好的，好，了解。

说话人 2 58:26 
然后你可以停止那个共享屏幕最后一个环节，就是你，你这边还有什么问题要问我。

说话人 1 58:33 
我这边想了解一下，我们这边是，就是主要做什么相关的一些业务，就业务相关啊。

说话人 2 58:38 
我们这边是这样的，就是商汤，有一个商汤产品，它其实是一个大语言模型的一个产品，可以把它理解为像我不知道你这有用过没有，他现在有一些这种语音对话文字的这种能力，其实可以理解，跟字节豆包和阅战案的那个 Timi 有点类似，有的作用功能不一样，然后它的底层的背后服务的这个大模型就是我们团队负责的，然后我们整个一个大团队分为两块，一个是多模态的，可能就是你明天要面的那个部门。

说话人 2 59:22 
嗯嗯，有可能，但具体哪个部门我不是很清楚，哎哎，但就是有一个团队的是做多模态的，也就他们会有一些视频和图像的数据。嗯，然后另外一块就是我们是底层的这个语言模型。然后这个语言模型我们团队的话它是全链路都会覆盖到。嗯，就比如说预训练SFT，包括做DPO、 PPO 这个强化学习的模训练都会有。然后支持的业务这块除了这个，最主要这个商量模型还有一些商。当当本身的一些跟企业合作的一些项目，比如说有些企业想要接入一些聊天对话系统，或者说是一些写代码的辅助都会去接入。嗯，好，了解了，一般来讲不会有对特别定制化的，专门去微调一个小模型给到服务方，一般都是我们只会针对一个题材模型去调。

说话人 1 01:00:26 
好的了解，嗯，那我想问一下，就说我们就说朱莉的人均卡计算资源充足吗？

说话人 2 01:00:33 
为什么充足啊？计算，因为我们现在整个公司大的语就是语言微调，就是就训练的团队，就我们组和另外一个多模态的组都是倾斜的。

说话人 1 01:00:50 
嗯，了解。嗯，我这边暂时没有其他问题了。好。

说话人 2 01:00:56 
的好的，就我刚刚，对，那我们今天就先这样。

说话人 1 01:01:01 
嗯，好嘞，谢谢您。好嘞好嘞。嗯嗯，好好。

说话人 2 01:01:03 
好，不客气，拜拜。

说话人 1 01:01:04 
诶，拜拜，好。

